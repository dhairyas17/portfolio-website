Modern machine learning isn’t just about training better models. It’s about delivering *reliable*, *versioned*, and *real-time* intelligence at scale.

Over the past year, I’ve worked closely with ML engineers, data annotators, and DevOps teams to take models from **Jupyter Notebooks** to **Jetson-powered edge deployments**, running real-time inference with **TensorRT** optimizations — all orchestrated via **Apache Airflow** and tracked through **MLflow**.

This is what I learned about the evolving MLOps stack.

---

## 🚦 From Offline Notebooks to Continuous Pipelines

Most ML teams begin with one-off experiments. But the moment you want reproducibility and consistency, you need structure. That’s where MLflow and Airflow came in.

- **MLflow** handled our versioning, metrics tracking, and artifact logging.
- **Airflow** helped us automate training, validation, conversion to ONNX/TensorRT, and deployment triggers.

> "Our pipeline wasn’t just CI/CD — it became CI/CT/CD: Continuous Integration, Continuous Testing, Continuous Deployment."

---

## ⚙️ Model Versioning Isn’t Optional

One early mistake: overwriting models during development. A small change in pre-processing broke downstream inference for days — and we had no way to trace it.

We moved to version-controlled models:

- Every MLflow run was tagged with metadata (dataset version, training params, preprocessing).
- Rollbacks became easy. Debugging became traceable.

---

## 🧠 The TensorRT Transition

Inference on Jetson devices demanded low-latency, GPU-optimized models.

- We converted PyTorch → ONNX → TensorRT.
- Batch size tuning and INT8 quantization gave us 3x speed-up.

This shift required close collaboration between MLOps engineers and researchers — especially around operator support and accuracy drift.

---

## 🔁 Real-Time Feedback Loops

Post-deployment, the work wasn’t done. We built a feedback loop using:

- **Airflow DAGs** that monitored inference failures or latency spikes.
- **Custom alerting** based on model confidence thresholds.
- Re-training triggers when distribution drift exceeded our KL divergence threshold.

This turned inference into an *active*, monitored service — not a black box.

---

## 📍 Stakeholder Mini-Stories

- **Ops team** flagged inconsistent inference latency on edge devices — leading us to dig into TensorRT engine warm-up behavior and preload it at boot.
- **PMs** pushed for more frequent model updates — we aligned via a model calendar synced with product sprints and QA cycles.
- **Data team** resisted retraining cadence — we introduced a precision-recall regression tracker to show actual impact of fresh data.

---

## 🧠 Pull-Quote

> **"In MLOps, shipping a model means shipping reliability twice — once in code, once in behavior."**

---

## 📈 Reflections and What I'd Do Differently

- Introduce experiment tracking **earlier** in the lifecycle — ideally Day 1.
- Build data validation gates into Airflow — to catch bad batches before training.
- Use modular ML components (e.g. feature stores, model registries) for reuse across projects.

---

## ✅ Final Takeaways

- MLOps isn’t a tool stack — it’s a mindset shift from experimentation to productization.
- Cross-functional fluency (research ↔ infra ↔ PM) is key to success.
- The best model isn’t the most accurate — it’s the most *maintainable* in production.

---
 