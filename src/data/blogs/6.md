Modern machine learning is not just about training better models. It is about delivering reliable, versioned, and real-time intelligence at scale. Over the past year, I worked with ML engineers, data annotators, and DevOps teams to take models from Jupyter notebooks to Jetson-powered edge deployments.

 These deployments ran real-time inference with TensorRT optimizations, orchestrated through Apache Airflow, and tracked with MLflow.

### From Offline Experiments to Continuous Pipelines

Many ML teams begin with one-off experiments. The moment reproducibility and consistency become priorities, structure is essential. MLflow handled versioning, metrics tracking, and artifact logging. 

Airflow automated the entire lifecycle: 
- training
- validation
- conversion to ONNX/TensorRT
- deployment triggers.

The result was more than CI/CD. It became CI/CT/CD, Continuous Integration, Continuous Testing, and Continuous Deployment.

### Why Model Versioning Cannot Be Optional

One early mistake was overwriting models during development. A minor preprocessing change broke downstream inference for days, with no traceability. We moved to a version-controlled approach:

- Each MLflow run included dataset version, training parameters, and preprocessing metadata  
- Rollbacks became fast and debugging became traceable

### The TensorRT Transition

Inference on Jetson devices required low-latency, GPU-optimized models.
We converted PyTorch models to ONNX, then to TensorRT. Batch size tuning and INT8 quantization gave a threefold speed increase. 

This process demanded tight collaboration between MLOps engineers and researchers, especially when dealing with operator support issues and guarding against accuracy drift.

### Building Real-Time Feedback Loops

Deployment was only the halfway point. We implemented:

- Airflow DAGs to monitor inference failures and latency spikes  
- Custom alerts based on model confidence thresholds  
- Retraining triggers when distribution drift exceeded a defined KL divergence threshold

Inference became an actively monitored service rather than a black box.

### Stakeholder Examples

The Ops team reported inconsistent inference latency on edge devices. Investigation revealed TensorRT engine warm-up delays, which we fixed by preloading at boot. 

Product managers requested more frequent updates, leading to a model release calendar aligned with sprints and QA cycles.  

The data team resisted the retraining cadence until we implemented a precisionâ€“recall regression tracker to demonstrate the tangible impact of fresh data.

### Quote

> In MLOps, shipping a model means shipping reliability twice, once in code, once in behavior.

### Reflections

If I could start again, I would:

- Introduce experiment tracking from day one  
- Build data validation gates into Airflow to catch bad batches before training  
- Use modular ML components such as feature stores and model registries for reuse across projects

### Final Takeaways

- MLOps is not just a tool stack, it is a mindset shift from experimentation to productization.  
- Cross-functional fluency between research, infrastructure, and product is critical.  
+ The best model is not necessarily the most accurate, it is the most maintainable in production.
