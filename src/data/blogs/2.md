As a TPM working on edge AI products, one of my biggest challenges was not just deciding what to build. It was deciding what not to build. When you are operating at the intersection of firmware, DevOps, machine learning, and user experience, prioritization becomes less of a roadmap exercise and more of a system design problem.

### The Chaos: Three Streams, One Roadmap

Every quarter brought a flood of requests from different functions:
- **Product and UX** wanted smarter alerts, real-time tagging, and smoother UI experiences.  
- **Firmware and Hardware** needed Jetson orin nano patching for new models and fixes for thermal issues.  
- **DevOps** flagged fragile CI/CD pipelines and non-reproducible model builds that required infrastructure upgrades.

All three perspectives were valid. But time was limited, edge devices were already deployed in the field, and quarterly OKRs were non-negotiable.

### Step 1: Triage with RICE and MoSCoW

We created a hybrid prioritization model that combined RICE scoring with MoSCoW categorization.

**RICE Framework**  
We scored each initiative based on:  
- Reach: How many customers or devices would it affect?  
- Impact: What effect would it have on performance, customer satisfaction, or conversions?  
- Confidence: Did we have supporting data or were we relying on assumptions?  
- Effort: Estimated in sprint points or team capacity.

**MoSCoW Layering**  
After scoring with RICE, we categorized using MoSCoW:  
- Must-Have: Directly tied to quarterly OKRs  
- Should-Have: Significant value but could be deferred  
- Could-Have: Nice to have, no immediate pressure  
- Will Not Have Now: Parked for future review

### Step 2: Structured Inputs with Productboard

We operationalized this process using Productboard.
- All feature ideas were tagged by type such as UX, infrastructure, or firmware.  
- Each idea was linked to customer feedback, support tickets, and internal objectives.  
- Weekly triage sessions with engineering teams ensured scores were updated as new information emerged.

### Real Trade-offs

We repeatedly faced choices such as whether to spend two weeks improving firmware upgrade reliability or refining the labeling UI for faster onboarding.
Some examples of the calls we made:

- Prioritized firmware OTA patching over a new UX feature because device downtime had a higher cost than UI friction.  
- Deferred model versioning infrastructure to address pilot user complaints that were risking churn.  
- Rewrote the alert module pipeline instead of acting on certain NPS feedback because DevOps stability had become a critical blocker.

### Stakeholder Snapshots

- Operations teams flagged device dropouts during updates, which led to prioritizing firmware rollback safety after joint testing.  
- Product manager pushed for a faster deployment UI, resulting in a deployment dashboard MVP even though engineering resources were tight.  
- Weekly three-way reviews between UX, ML, and engineering leads at the start of each sprint reduced back-and-forth delays.

### Sidebar Quote

> In edge products, shipping a feature means shipping reliability twice: once in the code, and again in the deployment.

### Reflection

If I ran this process again, I would:

- Build visual dashboards for real-time RICE and MoSCoW trade-offs rather than relying on spreadsheets and memory  
- Establish a parking lot review process for Could-Haves every four weeks  
- Improve at saying “no” with context so teams understood the decision

### Summary

- Prioritization in edge AI is not a checklist. 
- It is orchestration. 
- It is about aligning teams that operate at different rhythms, use 
different languages, and measure different outcomes. 
- Frameworks help, but as a TPM, the real work is building the trust that the hard trade-offs are the right ones.
